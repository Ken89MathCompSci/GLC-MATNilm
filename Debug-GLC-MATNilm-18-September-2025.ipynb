{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOuIdL3YEJLAW0kpUSsU1hE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ken89MathCompSci/GLC-MATNilm/blob/master/Debug-GLC-MATNilm-18-September-2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "VR8MABC3BZUI",
        "outputId": "806f19df-4f47-47dd-f362-1982cb120e0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/GLC-MATNilm'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd GLC-MATNilm/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlW9aNAmCdPg",
        "outputId": "5268b43c-76da-4ee2-bd5c-c1dd70eddc5a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'GLC-MATNilm/'\n",
            "/content/GLC-MATNilm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TqpeCuuoDcUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f586dab7"
      },
      "source": [
        "# Task\n",
        "Run the training script and save the output to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a160465"
      },
      "source": [
        "## Mount google drive\n",
        "\n",
        "### Subtask:\n",
        "Mount Google Drive to access files and save outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6eebb88"
      },
      "source": [
        "**Reasoning**:\n",
        "Mount Google Drive to access files and save outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9744257d",
        "outputId": "72444670-9e41-41ca-859a-77a7b10dab33"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80198ecb"
      },
      "source": [
        "## Install dependencies\n",
        "\n",
        "### Subtask:\n",
        "Install any necessary libraries or packages required for the training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0df5c8cb"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the contents of the current directory to determine the dependencies for the training script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4646a5e0",
        "outputId": "0b43b4da-28e0-4c94-fc97-31cb037cc776"
      },
      "source": [
        "import os\n",
        "print(os.listdir('/content'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.config', 'GLC-MATNilm', 'drive', 'sample_data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6a71997"
      },
      "source": [
        "**Reasoning**:\n",
        "Change the directory to 'GLC-MATNilm' to access the training script and potential requirements files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7fcc256",
        "outputId": "400e151f-4cfe-4b95-f48d-943d5b61e1e4"
      },
      "source": [
        "%cd /content/GLC-MATNilm"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GLC-MATNilm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "055724b1"
      },
      "source": [
        "**Reasoning**:\n",
        "List the files in the current directory to identify the training script and any requirements files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ba04ca",
        "outputId": "597ab19b-447e-4f27-e1c6-5f55378059f6"
      },
      "source": [
        "import os\n",
        "print(os.listdir('.'))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['loss_func.py', 'modules.py', '__pycache__', 'history_model', 'utils.py', '.git', 'sa_utils.py', 'custom_types.py', 'eval_utils.py', 'data', 'log', 'main.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e81af93"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the file listing, 'main.py' is likely the training script. There is no obvious requirements file. I will inspect 'main.py' for import statements to determine dependencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3d0cccb",
        "outputId": "ee2113b9-a898-425d-cd89-04dc12fde642"
      },
      "source": [
        "with open('main.py', 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import copy\n",
            "import os\n",
            "import utils\n",
            "import eval_utils\n",
            "import sa_utils\n",
            "import argparse\n",
            "import joblib\n",
            "from tqdm import tqdm\n",
            "import numpy as np\n",
            "import torch\n",
            "from torch import nn, optim\n",
            "from torch.utils.data import DataLoader\n",
            "from custom_types import Basic, TrainConfig\n",
            "from modules import MATconv as MAT\n",
            "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
            "import matplotlib.pyplot as plt\n",
            "import logging\n",
            "from loss_func import BERT4NILMLoss\n",
            "\n",
            "logging.getLogger('matplotlib').setLevel(logging.INFO)\n",
            "\n",
            "def get_args():\n",
            "    parser = argparse.ArgumentParser()\n",
            "    parser.add_argument(\"--batch\", type=int, default=32, help=\"batch size\")\n",
            "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
            "    parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout\")\n",
            "    parser.add_argument(\"--hidden\", type=int, default=32, help=\"encoder decoder hidden size\")\n",
            "    parser.add_argument(\"--logname\", action=\"store\", default='root', help=\"name for log\")\n",
            "    parser.add_argument(\"--subName\", action=\"store\", type=str, default='test', help=\"name of the directory of current run\")\n",
            "    parser.add_argument(\"--inputLength\", type=int, default=864, help=\"input length for the model\")\n",
            "    parser.add_argument(\"--outputLength\", type=int, default=864, help=\"output length for the model\")\n",
            "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debug mode\")\n",
            "    parser.add_argument(\"--dataAug\", action=\"store_true\", help=\"data augmentation mode\")\n",
            "    parser.add_argument(\"--prob0\", type=float, default=0.3, help=\"augment probability for Dishwasher\")\n",
            "    parser.add_argument(\"--prob1\", type=float, default=0.6, help=\"weight\")\n",
            "    parser.add_argument(\"--prob2\", type=float, default=0.3, help=\"weight\")\n",
            "    parser.add_argument(\"--prob3\", type=float, default=0.3, help=\"weight\")\n",
            "    parser.add_argument(\"--test\", action=\"store_true\", help=\"skip training and only run evaluation\")\n",
            "    parser.add_argument(\"--show_graph\", action=\"store_true\", help=\"plot prediction\")\n",
            "    return parser.parse_args()\n",
            "\n",
            "def train(t_net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=200):\n",
            "    iter_loss = []\n",
            "    vali_loss = []\n",
            "    early_stopping_all = utils.EarlyStopping(logger, patience=20, verbose=True)\n",
            "\n",
            "    if config.dataAug:\n",
            "        sigClass = sa_utils.sigGen(config)\n",
            "\n",
            "    path_all = os.path.join(modelDir, \"All_best_onoff.ckpt\")\n",
            "\n",
            "    for e_i in range(epo):\n",
            "        logger.info(f\"# of epoches: {e_i}\")\n",
            "        for t_i, (_, _, X_scaled, Y_scaled, Y_of) in enumerate(tqdm(train_Dataloader)):\n",
            "            if config.dataAug:\n",
            "                X_scaled, Y_scaled, Y_of = sa_utils.dataAug(X_scaled.clone(), Y_scaled.clone(), Y_of.clone(), sigClass, config)\n",
            "\n",
            "            t_net.model_opt.zero_grad(set_to_none=True)\n",
            "\n",
            "            X_scaled = X_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "            Y_scaled = Y_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "            Y_of = Y_of.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "\n",
            "            y_pred_dish_r, y_pred_dish_c = t_net.model(X_scaled)\n",
            "\n",
            "            # OLD LOSS\n",
            "            # loss_r = criterion[0](y_pred_dish_r,Y_scaled)\n",
            "            # loss_c = criterion[1](y_pred_dish_c, Y_of)\n",
            "\n",
            "            # loss=loss_r+loss_c\n",
            "            # loss.backward()\n",
            "\n",
            "            loss = criterion(y_pred_dish_r, Y_scaled, y_pred_dish_c, Y_of)\n",
            "            loss.backward()\n",
            "\n",
            "            t_net.model_opt.step()\n",
            "            iter_loss.append(loss.item())\n",
            "\n",
            "        epoch_losses = np.average(iter_loss)\n",
            "\n",
            "        logger.info(f\"Validation: \")\n",
            "        maeScore, y_vali_ori, y_vali_pred_d_update, _, _, _ = eval_utils.evaluateResult(net, config, vali_Dataloader, logger)\n",
            "        # val_loss = criterion[0](y_vali_ori, y_vali_pred_d_update)\n",
            "        val_loss = criterion(y_vali_ori, y_vali_pred_d_update)\n",
            "        logger.info(f\"Epoch {e_i:d}, train loss: {epoch_losses:3.3f}, val loss: {val_loss:3.3f}.\")\n",
            "        vali_loss.append(val_loss)\n",
            "\n",
            "        if e_i % 10 == 0:\n",
            "            checkpointName = os.path.join(modelDir, \"checkpoint_\" + str(e_i) + '.ckpt')\n",
            "            utils.saveModel(logger, net, checkpointName)\n",
            "\n",
            "        logger.info(f\"Early stopping overall: \")\n",
            "        early_stopping_all(np.mean(maeScore), net, path_all)\n",
            "        if early_stopping_all.early_stop:\n",
            "            print(\"Early stopping\")\n",
            "            break\n",
            "\n",
            "    net_all = copy.deepcopy(net)\n",
            "    checkpoint_all = torch.load(path_all, map_location=device)\n",
            "    utils.loadModel(logger, net_all, checkpoint_all)\n",
            "    net_all.model.eval()\n",
            "    return net_all\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    args = get_args()\n",
            "    utils.mkdir(\"log/\" + args.subName)\n",
            "    logger = utils.setup_log(args.subName, args.logname)\n",
            "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
            "    logger.info(f\"Using computation device: {device}\")\n",
            "    logger.info(args)\n",
            "    if args.debug:\n",
            "        epo = 2\n",
            "    else:\n",
            "        epo = 200\n",
            "\n",
            "    # Dataloder\n",
            "    logger.info(f\"loading data\")\n",
            "    train_data, val_data, test_data = utils.data_loader(args)\n",
            "    # aggregate, appliance 1, appliance 2, appliance 3, appliance 4\n",
            "\n",
            "    logger.info(f\"loading data finished\")\n",
            "\n",
            "    config_dict = {\n",
            "        \"input_size\": 1,\n",
            "        \"batch_size\": args.batch,\n",
            "        \"hidden\": args.hidden,\n",
            "        \"lr\": args.lr,\n",
            "        \"dropout\": args.dropout,\n",
            "        \"logname\": args.logname,\n",
            "        \"outputLength\": args.outputLength,\n",
            "        \"inputLength\" : args.inputLength,\n",
            "        \"subName\": args.subName,\n",
            "        \"dataAug\": args.dataAug,\n",
            "        \"prob0\": args.prob0,\n",
            "        \"prob1\": args.prob1,\n",
            "        \"prob2\": args.prob2,\n",
            "        \"prob3\": args.prob3,\n",
            "    }\n",
            "\n",
            "    config = TrainConfig.from_dict(config_dict)\n",
            "    modelDir = utils.mkdirectory(config.subName, saveModel=True)\n",
            "    joblib.dump(config, os.path.join(modelDir, \"config.pkl\"))\n",
            "\n",
            "    logger.info(f\"Training size: {train_data.cumulative_sizes[-1]:d}.\")\n",
            "\n",
            "    index = np.arange(0,train_data.cumulative_sizes[-1])\n",
            "    train_subsampler = torch.utils.data.SubsetRandomSampler(index)\n",
            "    train_Dataloader = DataLoader(\n",
            "        train_data,\n",
            "        batch_size=config.batch_size,\n",
            "        sampler=train_subsampler,\n",
            "        num_workers=4,\n",
            "        pin_memory=True)\n",
            "\n",
            "    sampler = utils.testSampler(val_data.cumulative_sizes[-1], config.outputLength)\n",
            "    sampler_test = utils.testSampler(test_data.cumulative_sizes[-1], config.outputLength)\n",
            "    # sampler len = 915678\n",
            "\n",
            "    vali_Dataloader = DataLoader(\n",
            "        val_data,\n",
            "        batch_size=config.batch_size,\n",
            "        sampler=sampler,\n",
            "        num_workers=4,\n",
            "        pin_memory=True)\n",
            "\n",
            "    test_Dataloader = DataLoader(\n",
            "        test_data,\n",
            "        batch_size=config.batch_size,\n",
            "        sampler=sampler_test,\n",
            "        num_workers=4,\n",
            "        pin_memory=True)\n",
            "\n",
            "    logger.info(\"Initialize model\")\n",
            "    model = MAT(config).to(device)\n",
            "    logger.info(\"Model MAT\")\n",
            "\n",
            "    optim = optim.Adam(params=[p for p in model.parameters() if p.requires_grad], lr=config.lr)\n",
            "    net = Basic(model, optim)\n",
            "    # criterion_r = nn.MSELoss()\n",
            "    # criterion_c = nn.BCELoss()\n",
            "    # criterion = [criterion_r, criterion_c]\n",
            "    criterion = BERT4NILMLoss(tau=0.1, lambda_=1.0)\n",
            "\n",
            "    if not args.test:\n",
            "        logger.info(\"Training start\")\n",
            "        net_all = train(net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=epo)\n",
            "        logger.info(\"Training end\")\n",
            "\n",
            "        logger.info(\"validation start\")\n",
            "        eval_utils.evaluateResult(net_all, config, vali_Dataloader, logger)\n",
            "        logger.info(\"test start\")\n",
            "        eval_utils.evaluateResult(net_all, config, test_Dataloader, logger)\n",
            "    else:\n",
            "        # Load model from checkpoint\n",
            "        checkpointPath = './history_model/test/s0/All_best_onoff.ckpt'\n",
            "\n",
            "        logger.info(f\"Loading model from checkpoint: {checkpointPath}\")\n",
            "        checkpoint = torch.load(checkpointPath, map_location=device)\n",
            "        utils.loadModel(logger, net, checkpoint)\n",
            "        net.model.eval()\n",
            "\n",
            "        logger.info(\"Running validation on loaded model\")\n",
            "        eval_utils.evaluateResult(net, config, vali_Dataloader, logger)\n",
            "        logger.info(\"Running test on loaded model\")\n",
            "        output = eval_utils.evaluateResult(net, config, test_Dataloader, logger)\n",
            "        mae, y_ori, y_pred, y_ori_c, y_pred_c, x = output\n",
            "        # y_ori, y_pred, y_ori_c, y_pred_c = ['dish washer', 'fridge', 'microwave', 'wash']\n",
            "\n",
            "        appliances = ['dish washer', 'fridge', 'microwave', 'wash']\n",
            "        tp_counts = {appliance: 0 for appliance in appliances}\n",
            "        tn_counts = {appliance: 0 for appliance in appliances}\n",
            "        fp_counts = {appliance: 0 for appliance in appliances}\n",
            "        fn_counts = {appliance: 0 for appliance in appliances}\n",
            "\n",
            "        # Convert the tensor outputs to numpy arrays\n",
            "        y_ori_np = y_ori.cpu().numpy() if torch.is_tensor(y_ori) else y_ori\n",
            "        y_pred_np = y_pred.cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
            "        y_ori_c_np = y_ori_c.cpu().numpy() if torch.is_tensor(y_ori_c) else y_ori_c\n",
            "        y_pred_c_np = y_pred_c.cpu().numpy() if torch.is_tensor(y_pred_c) else y_pred_c\n",
            "\n",
            "        data = []\n",
            "\n",
            "        header = \"x, \" + \", \".join([f\"{appliance}_ori, {appliance}_pred, {appliance}_ori_c, {appliance}_pred_c\" for appliance in appliances])\n",
            "        data.append(header)\n",
            "\n",
            "        for i in range(len(x)):\n",
            "            line = f\"{x[i]}, \" + \", \".join([f\"{y_ori_np[i][j]}, {y_pred_np[i][j]}, {y_ori_c_np[i][j]}, {y_pred_c_np[i][j]}\" for j in range(len(appliances))])\n",
            "            data.append(line)\n",
            "\n",
            "        # Write the data to a text file\n",
            "        txt_path = 'model_output.txt'\n",
            "        with open(txt_path, 'w') as f:\n",
            "            for line in data:\n",
            "                f.write(line + \"\\n\")\n",
            "\n",
            "        print(f\"Model output saved to {txt_path}\")\n",
            "\n",
            "        pred_threshold = 0.5\n",
            "        y_pred_bin = np.array([[1 if pred > pred_threshold else 0 for pred in appliance_preds] for appliance_preds in y_pred_c])\n",
            "\n",
            "        for i, appliance in enumerate(appliances):\n",
            "            y_true = np.array([x[i] for x in y_ori_c])\n",
            "            y_pred_value = y_pred_bin[:, i]\n",
            "            \n",
            "            tp_counts[appliance] = np.sum((y_true == 1) & (y_pred_value == 1))\n",
            "            tn_counts[appliance] = np.sum((y_true == 0) & (y_pred_value == 0))\n",
            "            fp_counts[appliance] = np.sum((y_true == 0) & (y_pred_value == 1))\n",
            "            fn_counts[appliance] = np.sum((y_true == 1) & (y_pred_value == 0))\n",
            "\n",
            "        for appliance in appliances:\n",
            "            print(f\"{appliance.capitalize()}:\")\n",
            "            print(f\"  True Positives: {tp_counts[appliance]}\")\n",
            "            print(f\"  True Negatives: {tn_counts[appliance]}\")\n",
            "            print(f\"  False Positives: {fp_counts[appliance]}\")\n",
            "            print(f\"  False Negatives: {fn_counts[appliance]}\")\n",
            "            print()\n",
            "\n",
            "        if args.show_graph:\n",
            "            appliances = ['Dish Washer', 'Fridge', 'Microwave', 'Wash']\n",
            "            y_ori_np = y_ori.cpu().numpy() if torch.is_tensor(y_ori) else y_ori\n",
            "            y_pred_np = y_pred.cpu().numpy() if torch.is_tensor(y_pred) else y_pred\n",
            "            fig, axes = plt.subplots(4, 1, figsize=(10, 10), sharex=True)\n",
            "            for i in range(4):\n",
            "                axes[i].plot(y_ori_np[:, i], label='Original', alpha=0.7)\n",
            "                axes[i].plot(y_pred_np[:, i], label='Predicted', alpha=0.7)\n",
            "                axes[i].set_title(appliances[i])\n",
            "                axes[i].legend()\n",
            "            plt.xlabel('Sample Index')\n",
            "            plt.ylabel('Value')\n",
            "            plt.suptitle('Original vs Predicted Values for Each Appliance')\n",
            "\n",
            "            # Show plot\n",
            "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
            "            plt.show()\n",
            "\n",
            "            # Plot classification results for each appliance\n",
            "            y_ori_c_np = y_ori_c.cpu().numpy() if torch.is_tensor(y_ori_c) else y_ori_c\n",
            "            y_pred_c_np = y_pred_c.cpu().numpy() if torch.is_tensor(y_pred_c) else y_pred_c\n",
            "            y_pred_c_binary = (y_pred_c_np > pred_threshold).astype(int)\n",
            "            fig, axes = plt.subplots(4, 1, figsize=(10, 10), sharex=True)\n",
            "            for i in range(4):\n",
            "                axes[i].plot(y_ori_c_np[:, i], label='Original Class', alpha=0.7)\n",
            "                axes[i].plot(y_pred_c_binary[:, i], label='Predicted Class', alpha=0.7)\n",
            "                axes[i].set_title(appliances[i])\n",
            "                axes[i].legend()\n",
            "            plt.xlabel('Sample Index')\n",
            "            plt.ylabel('Class')\n",
            "            plt.suptitle('Original vs Predicted Classification for Each Appliance')\n",
            "\n",
            "            plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
            "            plt.show()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9603ca73"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the import statements in 'main.py', the required libraries are `numpy`, `torch`, `tqdm`, `matplotlib`, `argparse`, and `joblib`. I will install them using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "e791d950",
        "outputId": "14481434-e8be-44d6-8e19-6ecfcd8b2fac"
      },
      "source": [
        "!pip install numpy torch tqdm matplotlib argparse joblib"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              },
              "id": "ac4b9e34661247d2b58eb84a1c2ed833"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aff3dc70"
      },
      "source": [
        "## Run training script\n",
        "\n",
        "### Subtask:\n",
        "Execute the training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41d8ffb9"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the main.py script to start the training process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da918736"
      },
      "source": [
        "# Task\n",
        "Run the training script and save checkpoints and intermediate trained models to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd4a1abe"
      },
      "source": [
        "## Modify training script\n",
        "\n",
        "### Subtask:\n",
        "Modify the training script to save checkpoints and intermediate models to a specified path in Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "215c934b"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main.py script to save checkpoints and the best model to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb4d03d0"
      },
      "source": [
        "with open('main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open('main.py', 'w') as f:\n",
        "    for line in lines:\n",
        "        if 'checkpointName = os.path.join(modelDir, \"checkpoint_\" + str(e_i) + \\'.ckpt\\')' in line:\n",
        "            f.write('        checkpointName = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"checkpoint_\" + str(e_i) + \\'.ckpt\\')\\n')\n",
        "        elif 'path_all = os.path.join(modelDir, \"All_best_onoff.ckpt\")' in line:\n",
        "             f.write('    path_all = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"All_best_onoff.ckpt\")\\n')\n",
        "        else:\n",
        "            f.write(line)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cc4e53a"
      },
      "source": [
        "## Run training script\n",
        "\n",
        "### Subtask:\n",
        "Execute the modified training script.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f33a768a"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the main.py script with the --test argument to avoid a long training process and quickly see the script's output and whether it saves files correctly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7212720",
        "outputId": "6e790f1c-28da-41bb-d3fd-ad1f659a0e0e"
      },
      "source": [
        "!python main.py --test"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/GLC-MATNilm/main.py\", line 89\n",
            "    checkpointName = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"checkpoint_\" + str(e_i) + '.ckpt')\n",
            "    ^^^^^^^^^^^^^^\n",
            "IndentationError: expected an indented block after 'if' statement on line 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c549e08a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed due to an IndentationError in the modified main.py script. I need to fix the indentation in the lines that were modified to save checkpoints and the best model to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "741dd92d"
      },
      "source": [
        "with open('main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open('main.py', 'w') as f:\n",
        "    for line in lines:\n",
        "        if 'checkpointName = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"checkpoint_\" + str(e_i) + \\'.ckpt\\')' in line:\n",
        "            f.write('            checkpointName = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"checkpoint_\" + str(e_i) + \\'.ckpt\\')\\n')\n",
        "        elif 'path_all = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"All_best_onoff.ckpt\")' in line:\n",
        "             f.write('    path_all = os.path.join(\"/content/drive/MyDrive/GLC-MATNilm_checkpoints\", \"All_best_onoff.ckpt\")\\n')\n",
        "        else:\n",
        "            f.write(line)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f28a7fd"
      },
      "source": [
        "**Reasoning**:\n",
        "The indentation errors in main.py should now be fixed. I will attempt to run the script again with the --test argument to confirm the fix and see if the script executes successfully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5da4edcc",
        "outputId": "0ea6a9e4-b665-4c7b-ff09-0d1d241ae761"
      },
      "source": [
        "!python main.py --test"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 13:43:39,386 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-18 13:43:39,386 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='test', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, test=True, show_graph=False)\n",
            "2025-09-18 13:43:39,387 - root - INFO - loading data\n",
            "2025-09-18 13:43:39,428 - root - INFO - loading data finished\n",
            "2025-09-18 13:43:39,432 - root - INFO - Training size: 27937.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-09-18 13:43:39,435 - root - INFO - Initialize model\n",
            "2025-09-18 13:43:39,807 - root - INFO - Model MAT\n",
            "2025-09-18 13:43:44,347 - root - INFO - Loading model from checkpoint: ./history_model/test/s0/All_best_onoff.ckpt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 198, in <module>\n",
            "    checkpoint = torch.load(checkpointPath, map_location=device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
            "    with _open_file_like(f, \"rb\") as opened_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
            "    super().__init__(open(name, mode))\n",
            "                     ^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: './history_model/test/s0/All_best_onoff.ckpt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80ac5794"
      },
      "source": [
        "**Reasoning**:\n",
        "The script failed because it could not find the checkpoint file at `./history_model/test/s0/All_best_onoff.ckpt`. This is expected in test mode if no prior training has occurred to create this file. To complete the subtask of executing the script, I will run it without the `--test` flag to initiate the training process, which will create the necessary checkpoint files. I will also specify a `subName` for the output directory in Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11cb12af",
        "outputId": "5d671784-351b-43c6-b193-75cdea3b24d5"
      },
      "source": [
        "!python main.py --subName colab_train"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 107, in <module>\n",
            "    logger = utils.setup_log(args.subName, args.logname)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/GLC-MATNilm/utils.py\", line 30, in setup_log\n",
            "    fh = logging.handlers.RotatingFileHandler(log_path, mode='a', maxBytes=100 * 1024 * 1024, backupCount=1, encoding='utf-8')\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 155, in __init__\n",
            "    BaseRotatingHandler.__init__(self, filename, mode, encoding=encoding,\n",
            "  File \"/usr/lib/python3.12/logging/handlers.py\", line 58, in __init__\n",
            "    logging.FileHandler.__init__(self, filename, mode=mode,\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1231, in __init__\n",
            "    StreamHandler.__init__(self, self._open())\n",
            "                                 ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/logging/__init__.py\", line 1263, in _open\n",
            "    return open_func(self.baseFilename, self.mode,\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/GLC-MATNilm/log/colab_train/rootlog_2025_09_18.log'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c61a4b2"
      },
      "source": [
        "**Reasoning**:\n",
        "The script failed because the log directory `/content/GLC-MATNilm/log/colab_train/` does not exist. The script attempts to create this directory but the `utils.mkdir` function likely doesn't create parent directories. I need to create the required log directory before running the script. I will create the directory `/content/GLC-MATNilm/log/colab_train` using `os.makedirs` which creates intermediate directories as needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3f1dee8"
      },
      "source": [
        "import os\n",
        "os.makedirs('/content/GLC-MATNilm/log/colab_train', exist_ok=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2c00423"
      },
      "source": [
        "**Reasoning**:\n",
        "The provided text contains a command `code_block` and includes Python code `!python main.py --subName colab_train`. The expectation describes the outcome of running this code, making it a HasCode type.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c17beb61",
        "outputId": "361de420-7c60-46c4-a27a-4ccac23fad03"
      },
      "source": [
        "!python main.py --subName colab_train"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 13:44:31,514 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-18 13:44:31,514 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='colab_train', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, test=False, show_graph=False)\n",
            "2025-09-18 13:44:31,514 - root - INFO - loading data\n",
            "2025-09-18 13:44:31,546 - root - INFO - loading data finished\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 142, in <module>\n",
            "    joblib.dump(config, os.path.join(modelDir, \"config.pkl\"))\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/joblib/numpy_pickle.py\", line 599, in dump\n",
            "    with open(filename, \"wb\") as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'history_model/colab_train/s0/config.pkl'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "436d123f"
      },
      "source": [
        "**Reasoning**:\n",
        "The script failed because the model history directory `/content/GLC-MATNilm/history_model/colab_train/s0/` does not exist and `joblib.dump` cannot create the parent directories. The script attempts to create this directory using `utils.mkdirectory`, but similar to the logging issue, it likely doesn't create parent directories. I need to create the required model history directory before running the script. I will create the directory `/content/GLC-MATNilm/history_model/colab_train/s0/` using `os.makedirs` which creates intermediate directories as needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e9e1e5e"
      },
      "source": [
        "import os\n",
        "os.makedirs('/content/GLC-MATNilm/history_model/colab_train/s0/', exist_ok=True)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66b54817"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that both the log and model history directories are created, I will attempt to run the main.py script again without the `--test` flag and with the `--subName colab_train` argument to initiate the training process and save outputs to the correct location in Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46b7ff93",
        "outputId": "4d88b0bc-a0c3-4c0f-ef34-58ea79a73376"
      },
      "source": [
        "!python main.py --subName colab_train"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 13:45:14,833 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-18 13:45:14,833 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='colab_train', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, test=False, show_graph=False)\n",
            "2025-09-18 13:45:14,834 - root - INFO - loading data\n",
            "2025-09-18 13:45:14,863 - root - INFO - loading data finished\n",
            "mkdir: cannot create directory ‘datacolab_train’: File exists\n",
            "2025-09-18 13:45:14,866 - root - INFO - Training size: 27937.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-09-18 13:45:14,867 - root - INFO - Initialize model\n",
            "2025-09-18 13:45:15,144 - root - INFO - Model MAT\n",
            "2025-09-18 13:45:16,050 - root - INFO - Training start\n",
            "2025-09-18 13:45:16,051 - root - INFO - # of epoches: 0\n",
            "  0% 0/874 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 874/874 [14:25<00:00,  1.01it/s]\n",
            "2025-09-18 13:59:41,481 - root - INFO - Validation: \n",
            "2025-09-18 13:59:41,997 - root - INFO - Evaluate dish washer: \n",
            "2025-09-18 13:59:41,998 - root - INFO - MAE: 22.833106994628906\n",
            "2025-09-18 13:59:41,998 - root - INFO - SAE: 22.257423400878906\n",
            "2025-09-18 13:59:42,016 - root - INFO - F1: 0.0\n",
            "2025-09-18 13:59:42,016 - root - INFO - Evaluate fridge: \n",
            "2025-09-18 13:59:42,016 - root - INFO - MAE: 56.842308044433594\n",
            "2025-09-18 13:59:42,017 - root - INFO - SAE: 27.898666381835938\n",
            "2025-09-18 13:59:42,021 - root - INFO - F1: 0.0\n",
            "2025-09-18 13:59:42,021 - root - INFO - Evaluate microwave: \n",
            "2025-09-18 13:59:42,021 - root - INFO - MAE: 19.437149047851562\n",
            "2025-09-18 13:59:42,022 - root - INFO - SAE: 20.014020919799805\n",
            "2025-09-18 13:59:42,026 - root - INFO - F1: 0.022779043280182234\n",
            "2025-09-18 13:59:42,027 - root - INFO - Evaluate wash: \n",
            "2025-09-18 13:59:42,027 - root - INFO - MAE: 286.41290283203125\n",
            "2025-09-18 13:59:42,027 - root - INFO - SAE: 295.3886413574219\n",
            "2025-09-18 13:59:42,032 - root - INFO - F1: 0.7313389485913447\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 186, in <module>\n",
            "    net_all = train(net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=epo)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 84, in train\n",
            "    val_loss = criterion(y_vali_ori, y_vali_pred_d_update)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "TypeError: BERT4NILMLoss.forward() missing 2 required positional arguments: 'y_pred_c' and 'y_true_c'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9f03d8df"
      },
      "source": [
        "## Run training script\n",
        "\n",
        "### Subtask:\n",
        "Retry running the modified training script after addressing the TypeError in the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87ba87ed"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the BERT4NILMLoss class in loss_func.py and the train function in main.py to understand the arguments expected by the loss function and those being passed in the train function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "454c8824",
        "outputId": "9851ca4a-8525-4f7b-887d-c20c67302dec"
      },
      "source": [
        "!python main.py --subName colab_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 14:19:12,608 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-18 14:19:12,608 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='colab_train', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, test=False, show_graph=False)\n",
            "2025-09-18 14:19:12,608 - root - INFO - loading data\n",
            "2025-09-18 14:19:12,642 - root - INFO - loading data finished\n",
            "mkdir: cannot create directory ‘datacolab_train’: File exists\n",
            "2025-09-18 14:19:12,645 - root - INFO - Training size: 27937.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-09-18 14:19:12,646 - root - INFO - Initialize model\n",
            "2025-09-18 14:19:12,942 - root - INFO - Model MAT\n",
            "2025-09-18 14:19:14,026 - root - INFO - Training start\n",
            "2025-09-18 14:19:14,026 - root - INFO - # of epoches: 0\n",
            "  0% 0/874 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "  4% 36/874 [00:38<14:05,  1.01s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fadbd147"
      },
      "source": [
        "## Debugging `NameError` in validation loss calculation\n",
        "\n",
        "### Subtask:\n",
        "Examine the `eval_utils.evaluateResult` function to understand its return values and identify the source of the `NameError`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7baedb7"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the `eval_utils.py` file to see the definition of the `evaluateResult` function and determine what values it returns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68b88b1d",
        "outputId": "215de619-c917-4f04-d670-742b055ff7cf"
      },
      "source": [
        "with open('eval_utils.py', 'r') as f:\n",
        "    print(f.read())"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import numpy as np\n",
            "from sklearn.metrics import f1_score\n",
            "\n",
            "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
            "\n",
            "def evaluate_score(y_real, y_predict, y_real_c, y_pred_c, logger):\n",
            "    maeScore = np.mean(np.abs(y_predict - y_real))\n",
            "    logger.info(f\"MAE: {maeScore}\")\n",
            "\n",
            "    num_period = int(len(y_real) / 1200)\n",
            "    diff = 0\n",
            "    for i in range(num_period):\n",
            "        diff += abs(np.sum(y_real[i * 1200: (i + 1) * 1200]) - np.sum(y_predict[i * 1200: (i + 1) * 1200]))\n",
            "    SAE = diff / (1200 * num_period)\n",
            "    logger.info(f\"SAE: {SAE}\")\n",
            "\n",
            "    f1s = f1_score(y_real_c, np.round(y_pred_c))\n",
            "    logger.info(f\"F1: {f1s}\")\n",
            "\n",
            "    return maeScore\n",
            "\n",
            "def evaluate_score_multi(y_real, y_predict, y_real_c, y_pred_c, logger):\n",
            "    listOfAppliance = ['dish washer', 'fridge', 'microwave', 'wash']\n",
            "    mapeScore = []\n",
            "    for i in range(y_predict.shape[1]):\n",
            "        logger.info(f\"Evaluate {listOfAppliance[i]}: \")\n",
            "        mapeScore.append(evaluate_score(y_real[:,i], y_predict[:,i], y_real_c[:,i], y_pred_c[:,i], logger))\n",
            "    return mapeScore\n",
            "    \n",
            "def evaluateResult(net, config, vali_Dataloader, logger, mode=-1):\n",
            "    y_vali_pred, y_vali, y_vali_ori, y_vali_pred_c, y_vali_ori_c, truex = predict(net, config, vali_Dataloader, mode=mode)\n",
            "    y_vali_pred = y_vali_pred.reshape(-1,y_vali_pred.shape[-1])\n",
            "    y_vali_pred_c = y_vali_pred_c.reshape(-1,y_vali_pred.shape[-1])\n",
            "    y_vali_ori = y_vali_ori.reshape(-1,y_vali_pred.shape[-1])\n",
            "    y_vali_ori_c = y_vali_ori_c.reshape(-1,y_vali_pred.shape[-1])\n",
            "    y_vali_pred[y_vali_pred < 0] = 0\n",
            "    y_vali_pred_d = y_vali_pred * 612\n",
            "\n",
            "    if mode >= 0:\n",
            "        maeScore = evaluate_score(y_vali_ori.numpy(), y_vali_pred_d.numpy(), y_vali_ori_c.numpy(), y_vali_pred_c.numpy(), logger)\n",
            "    else:\n",
            "        maeScore = evaluate_score_multi(y_vali_ori.numpy(), y_vali_pred_d.numpy(), y_vali_ori_c.numpy(), y_vali_pred_c.numpy(), logger)\n",
            "    return maeScore, y_vali_ori, y_vali_pred_d, y_vali_ori_c, y_vali_pred_c, truex.reshape(-1, 1)\n",
            "\n",
            "def predict(t_net, t_cfg, vali_Dataloader, mode=-1):\n",
            "    y_pred_r = []\n",
            "    y_true_scaled_r = []\n",
            "    y_true_r = []\n",
            "    y_pred_c = []\n",
            "    y_true_c = []\n",
            "    x_true = []\n",
            "    \n",
            "    start = int((t_cfg.inputLength-t_cfg.outputLength)/2)\n",
            "    end = start + t_cfg.outputLength\n",
            "\n",
            "    with torch.no_grad():\n",
            "        for _, (X, Y, X_scaled, Y_scaled, Y_of) in enumerate(vali_Dataloader):\n",
            "            if mode>=0:\n",
            "                Y = Y[:,start:end,[mode]]\n",
            "                Y_scaled = Y_scaled[:,start:end,[mode]]\n",
            "                Y_of = Y_of[:,start:end,[mode]]\n",
            "            else:\n",
            "                Y = Y[:,start:end,:]\n",
            "                Y_scaled = Y_scaled[:,start:end,:]\n",
            "                Y_of = Y_of[:,start:end,:]\n",
            "\n",
            "            X_scaled = X_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "            Y_scaled = Y_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "            Y = Y.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "            Y_of = Y_of.type(torch.FloatTensor).to(device, non_blocking=True)\n",
            "\n",
            "            output_r, output_c = t_net.model(X_scaled)\n",
            "            y_pred_r.append(output_r.cpu())\n",
            "            y_true_scaled_r.append(Y_scaled.cpu())\n",
            "            y_true_r.append(Y.cpu())\n",
            "            x_true.append(X[:,start:end,:])\n",
            "\n",
            "            y_pred_c.append(output_c.cpu())\n",
            "            y_true_c.append(Y_of.cpu())\n",
            "\n",
            "        out_pred_scaled_r = torch.vstack(y_pred_r)\n",
            "        out_true_scaled_r = torch.vstack(y_true_scaled_r)\n",
            "        out_true_r = torch.vstack(y_true_r)\n",
            "        out_true_x = torch.vstack(x_true)\n",
            "        out_pred_scaled_c = torch.vstack(y_pred_c)\n",
            "        out_true_c = torch.vstack(y_true_c)\n",
            "    return out_pred_scaled_r, out_true_scaled_r, out_true_r, out_pred_scaled_c, out_true_c, out_true_x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3195a2a9"
      },
      "source": [
        "**Reasoning**:\n",
        "The indentation errors in main.py should now be fixed. I will attempt to run the script again with the --test argument to confirm the fix and see if the script executes successfully."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b38db87",
        "outputId": "e73a9906-c8c2-4378-fcf1-e3cd940a10ff"
      },
      "source": [
        "!python main.py --subName colab_train"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-09-18 14:02:45,232 - root - INFO - Using computation device: cuda:0\n",
            "2025-09-18 14:02:45,232 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='colab_train', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, test=False, show_graph=False)\n",
            "2025-09-18 14:02:45,232 - root - INFO - loading data\n",
            "2025-09-18 14:02:45,263 - root - INFO - loading data finished\n",
            "mkdir: cannot create directory ‘datacolab_train’: File exists\n",
            "2025-09-18 14:02:45,266 - root - INFO - Training size: 27937.\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "2025-09-18 14:02:45,267 - root - INFO - Initialize model\n",
            "2025-09-18 14:02:45,524 - root - INFO - Model MAT\n",
            "2025-09-18 14:02:46,427 - root - INFO - Training start\n",
            "2025-09-18 14:02:46,427 - root - INFO - # of epoches: 0\n",
            "  0% 0/874 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "100% 874/874 [14:26<00:00,  1.01it/s]\n",
            "2025-09-18 14:17:13,426 - root - INFO - Validation: \n",
            "2025-09-18 14:17:14,013 - root - INFO - Evaluate dish washer: \n",
            "2025-09-18 14:17:14,014 - root - INFO - MAE: 22.83270263671875\n",
            "2025-09-18 14:17:14,015 - root - INFO - SAE: 23.545289993286133\n",
            "2025-09-18 14:17:14,023 - root - INFO - F1: 0.0\n",
            "2025-09-18 14:17:14,023 - root - INFO - Evaluate fridge: \n",
            "2025-09-18 14:17:14,024 - root - INFO - MAE: 54.98159408569336\n",
            "2025-09-18 14:17:14,024 - root - INFO - SAE: 35.16594696044922\n",
            "2025-09-18 14:17:14,030 - root - INFO - F1: 0.0\n",
            "2025-09-18 14:17:14,030 - root - INFO - Evaluate microwave: \n",
            "2025-09-18 14:17:14,031 - root - INFO - MAE: 19.437149047851562\n",
            "2025-09-18 14:17:14,031 - root - INFO - SAE: 20.014020919799805\n",
            "2025-09-18 14:17:14,037 - root - INFO - F1: 0.0\n",
            "2025-09-18 14:17:14,037 - root - INFO - Evaluate wash: \n",
            "2025-09-18 14:17:14,038 - root - INFO - MAE: 291.8457946777344\n",
            "2025-09-18 14:17:14,038 - root - INFO - SAE: 298.848388671875\n",
            "2025-09-18 14:17:14,046 - root - INFO - F1: 0.0\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 186, in <module>\n",
            "    net_all = train(net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=epo)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/GLC-MATNilm/main.py\", line 84, in train\n",
            "    val_loss = criterion(y_vali_ori, y_vali_pred_d_update, y_vali_ori_c, y_vali_pred_c)\n",
            "                                                           ^^^^^^^^^^^^\n",
            "NameError: name 'y_vali_ori_c' is not defined. Did you mean: 'y_vali_ori'?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ffd5e93"
      },
      "source": [
        "## Run training script\n",
        "\n",
        "### Subtask:\n",
        "Retry running the modified training script after addressing the TypeError in the loss function.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acdf686c"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the BERT4NILMLoss class in loss_func.py and the train function in main.py to understand the arguments expected by the loss function and those being passed in the train function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9bd30c2",
        "outputId": "bbb36d8d-bc49-45f2-b29a-73307c618653"
      },
      "source": [
        "with open('loss_func.py', 'r') as f:\n",
        "    print(f.read())\n",
        "with open('main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "with open('main.py', 'w') as f:\n",
        "    for line in lines:\n",
        "        if 'maeScore, y_vali_ori, y_vali_pred_d_update, _, _, _ = eval_utils.evaluateResult(net, config, vali_Dataloader, logger)' in line:\n",
        "            f.write('        maeScore, y_vali_ori, y_vali_pred_d_update, y_vali_ori_c, y_vali_pred_c, _ = eval_utils.evaluateResult(net, config, vali_Dataloader, logger)\\n')\n",
        "        elif 'val_loss = criterion(y_vali_ori, y_vali_pred_d_update)' in line:\n",
        "             f.write('        val_loss = criterion(y_vali_ori, y_vali_pred_d_update, y_vali_ori_c, y_vali_pred_c)\\n')\n",
        "        else:\n",
        "            f.write(line)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import torch\n",
            "import torch.nn as nn\n",
            "import torch.nn.functional as F\n",
            "\n",
            "class BERT4NILMLoss(nn.Module):\n",
            "    def __init__(self, tau=0.1, lambda_=1.0):\n",
            "        super(BERT4NILMLoss, self).__init__()\n",
            "        self.tau = tau\n",
            "        self.lambda_ = lambda_\n",
            "        self.criterion_r = nn.MSELoss()\n",
            "        self.criterion_c = nn.BCELoss()\n",
            "\n",
            "    def forward(self, y_pred_r, y_true_r, y_pred_c, y_true_c):\n",
            "        # Mean Squared Error Loss\n",
            "        mse_loss = self.criterion_r(y_pred_r, y_true_r)\n",
            "        \n",
            "        # KL Divergence Loss\n",
            "        softmax_pred = F.softmax(y_pred_r / self.tau, dim=-1)\n",
            "        softmax_true = F.softmax(y_true_r / self.tau, dim=-1)\n",
            "        kl_loss = F.kl_div(softmax_pred.log(), softmax_true, reduction='batchmean')\n",
            "        \n",
            "        # Soft-Margin Loss\n",
            "        soft_margin_loss = torch.mean(torch.log(1 + torch.exp(-y_true_c * y_pred_c)))\n",
            "        \n",
            "        # L1 Loss Term\n",
            "        l1_loss = torch.mean(torch.abs(y_pred_r - y_true_r) * (y_true_c > 0.5).float())\n",
            "\n",
            "        # Combined Loss\n",
            "        total_loss = mse_loss + kl_loss + soft_margin_loss + self.lambda_ * l1_loss\n",
            "        \n",
            "        return total_loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7981550"
      },
      "source": [
        "**Reasoning**:\n",
        "The output shows that `BERT4NILMLoss.forward` expects `y_pred_r`, `y_true_r`, `y_pred_c`, and `y_true_c`. The `train` function calls `criterion` with `y_pred_dish_r`, `Y_scaled`, `y_pred_dish_c`, and `Y_of` during training, which matches the expected arguments. However, during validation, it calls `criterion` with only `y_vali_ori` and `y_vali_pred_d_update`, which are likely the regression outputs and are missing the classification outputs. I will modify the validation call to include the classification outputs.\n",
        "\n"
      ]
    }
  ]
}