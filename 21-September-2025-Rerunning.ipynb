{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ken89MathCompSci/GLC-MATNilm/blob/kengoh-learnable-positional-encoding-only/21-September-2025-Rerunning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5YZH5ETz8gLD",
        "outputId": "abb9de46-3fd8-4030-c448-8bb8374b11b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TkbsscECL2s",
        "outputId": "1d569910-7b8c-419d-9c5f-278a42f99e16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GLC-MATNilm\n"
          ]
        }
      ],
      "source": [
        "cd GLC-MATNilm/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpfgJ6E8Cw5A",
        "outputId": "11508e60-b5f3-4ddf-dd55-4bbd0232a638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aIvMpz1CymF",
        "outputId": "99aaebff-463c-444e-cb46-9a55c31a1478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GLC-MATNilm\n"
          ]
        }
      ],
      "source": [
        "%cd /content/GLC-MATNilm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XjT6NJtdC3gf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/content/GLC-MATNilm/history_model/colab_train/s0', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R5kvE5QJC5nu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs('/content/GLC-MATNilm/log/colab_train', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7xB7Qi7C6Gb",
        "outputId": "e88e87dc-a137-4309-8ae7-6ed5354713a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: main.py [-h] [--batch BATCH] [--lr LR] [--dropout DROPOUT]\n",
            "               [--hidden HIDDEN] [--logname LOGNAME] [--subName SUBNAME]\n",
            "               [--inputLength INPUTLENGTH] [--outputLength OUTPUTLENGTH]\n",
            "               [--debug] [--dataAug] [--prob0 PROB0] [--prob1 PROB1]\n",
            "               [--prob2 PROB2] [--prob3 PROB3] [--resume]\n",
            "               [--checkpoint CHECKPOINT]\n",
            "main.py: error: unrecognized arguments: --output_dir /content/drive/My Drive/GLC-MATNilm_results\n"
          ]
        }
      ],
      "source": [
        "!python main.py --dataAug --subName colab_train --output_dir \"/content/drive/My Drive/GLC-MATNilm_results\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27407c5d",
        "outputId": "f5f56860-ae73-4223-9cdc-f2dc787faf6e"
      },
      "source": [
        "import copy\n",
        "import os\n",
        "import utils\n",
        "import argparse\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from custom_types import Basic, TrainConfig\n",
        "from modules import MATconv as MAT\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "import sys # Import the sys module\n",
        "\n",
        "\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--batch\", type=int, default=32, help=\"batch size\")\n",
        "    parser.add_argument(\"--lr\", type=float, default=0.001, help=\"learning rate\")\n",
        "    parser.add_argument(\"--dropout\", type=float, default=0.1, help=\"dropout\")\n",
        "    parser.add_argument(\"--hidden\", type=int, default=32, help=\"encoder decoder hidden size\")\n",
        "    parser.add_argument(\"--logname\", action=\"store\", default='root', help=\"name for log\")\n",
        "    parser.add_argument(\"--subName\", action=\"store\", type=str, default='test', help=\"name of the directory of current run\")\n",
        "    parser.add_argument(\"--inputLength\", type=int, default=864, help=\"input length for the model\")\n",
        "    parser.add_argument(\"--outputLength\", type=int, default=864, help=\"output length for the model\")\n",
        "    parser.add_argument(\"--debug\", action=\"store_true\", help=\"debug mode\")\n",
        "    parser.add_argument(\"--dataAug\", action=\"store_true\", help=\"data augmentation mode\")\n",
        "    parser.add_argument(\"--prob0\", type=float, default=0.3, help=\"augment probability for Dishwasher\")\n",
        "    parser.add_argument(\"--prob1\", type=float, default=0.6, help=\"weight\")\n",
        "    parser.add_argument(\"--prob2\", type=float, default=0.3, help=\"weight\")\n",
        "    parser.add_argument(\"--prob3\", type=float, default=0.3, help=\"weight\")\n",
        "    parser.add_argument(\"--resume\", action=\"store_true\", help=\"resume training from checkpoint\")\n",
        "    parser.add_argument(\"--checkpoint\", type=str, default=\"All_best_onoff.ckpt\", help=\"checkpoint file name to resume from\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\".\", help=\"directory to save output results\") # Added output_dir argument\n",
        "\n",
        "    # Workaround for Colab's kernel launcher adding extra arguments\n",
        "    if '__file__' not in globals():\n",
        "        sys.argv = [sys.argv[0]]\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "def train(t_net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=200):\n",
        "    iter_loss = []\n",
        "    vali_loss = []\n",
        "    early_stopping_all = utils.EarlyStopping(logger, patience=30, verbose=True)\n",
        "\n",
        "    if config.dataAug:\n",
        "        sigClass = utils.sigGen(config)\n",
        "\n",
        "    path_all = os.path.join(modelDir, \"All_best_onoff.ckpt\")\n",
        "\n",
        "    for e_i in range(epo):\n",
        "\n",
        "        logger.info(f\"# of epoches: {e_i}\")\n",
        "        for t_i, (_, _, X_scaled, Y_scaled, Y_of) in enumerate(tqdm(train_Dataloader)):\n",
        "            if config.dataAug:\n",
        "                X_scaled, Y_scaled, Y_of = utils.dataAug(X_scaled.clone(), Y_scaled.clone(), Y_of.clone(), sigClass, config)\n",
        "\n",
        "            t_net.model_opt.zero_grad(set_to_none=True)\n",
        "\n",
        "            X_scaled = X_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "            Y_scaled = Y_scaled.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "            Y_of = Y_of.type(torch.FloatTensor).to(device, non_blocking=True)\n",
        "\n",
        "            y_pred_dish_r, y_pred_dish_c = t_net.model(X_scaled)\n",
        "\n",
        "            loss_r = criterion[0](y_pred_dish_r,Y_scaled)\n",
        "            loss_c = criterion[1](y_pred_dish_c, Y_of)\n",
        "\n",
        "            loss=loss_r+loss_c\n",
        "            loss.backward()\n",
        "\n",
        "            t_net.model_opt.step()\n",
        "            iter_loss.append(loss.item())\n",
        "\n",
        "        epoch_losses = np.average(iter_loss)\n",
        "\n",
        "        logger.info(f\"Validation: \")\n",
        "        maeScore, y_vali_ori, y_vali_pred_d_update, _, _, _ = utils.evaluateResult(net, config, vali_Dataloader, logger)\n",
        "        val_loss = criterion[0](y_vali_ori, y_vali_pred_d_update)\n",
        "        logger.info(f\"Epoch {e_i:d}, train loss: {epoch_losses:3.3f}, val loss: {val_loss:3.3f}.\")\n",
        "        vali_loss.append(val_loss)\n",
        "\n",
        "        if e_i % 10 == 0:\n",
        "            checkpointName = os.path.join(modelDir, \"checkpoint_\" + str(e_i) + '.ckpt')\n",
        "            utils.saveModel(logger, net, checkpointName)\n",
        "\n",
        "        logger.info(f\"Early stopping overall: \")\n",
        "        early_stopping_all(np.mean(maeScore), net, path_all)\n",
        "        if early_stopping_all.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    net_all = copy.deepcopy(net)\n",
        "    checkpoint_all = torch.load(path_all, map_location=device)\n",
        "    utils.loadModel(logger, net_all, checkpoint_all)\n",
        "    net_all.model.eval()\n",
        "\n",
        "    return net_all\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = get_args()\n",
        "    utils.mkdir(\"log/\" + args.subName)\n",
        "    logger = utils.setup_log(args.subName, args.logname)\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    logger.info(f\"Using computation device: {device}\")\n",
        "    logger.info(args)\n",
        "    if args.debug:\n",
        "        epo = 2\n",
        "    else:\n",
        "        epo = 200\n",
        "\n",
        "    # splitLoss = False\n",
        "    # trainFull = True\n",
        "\n",
        "    # Dataloder\n",
        "    logger.info(f\"loading data\")\n",
        "    train_data, val_data, test_data = utils.data_loader(args)\n",
        "\n",
        "    logger.info(f\"loading data finished\")\n",
        "\n",
        "    config_dict = {\n",
        "        \"input_size\": 1,\n",
        "        \"batch_size\": args.batch,\n",
        "        \"hidden\": args.hidden,\n",
        "        \"lr\": args.lr,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"logname\": args.logname,\n",
        "        \"outputLength\": args.outputLength,\n",
        "        \"inputLength\" : args.inputLength,\n",
        "        \"subName\": args.subName,\n",
        "        \"dataAug\": args.dataAug,\n",
        "        \"prob0\": args.prob0,\n",
        "        \"prob1\": args.prob1,\n",
        "        \"prob2\": args.prob2,\n",
        "        \"prob3\": args.prob3,\n",
        "        \"output_dir\": args.output_dir # Added to config\n",
        "    }\n",
        "\n",
        "    config = TrainConfig.from_dict(config_dict)\n",
        "    modelDir = utils.mkdirectory(config.subName, saveModel=True) # This needs to be updated to use output_dir\n",
        "    joblib.dump(config, os.path.join(modelDir, \"config.pkl\"))\n",
        "\n",
        "\n",
        "    logger.info(f\"Training size: {train_data.cumulative_sizes[-1]:d}.\")\n",
        "\n",
        "    index = np.arange(0,train_data.cumulative_sizes[-1])\n",
        "    train_subsampler = torch.utils.data.SubsetRandomSampler(index)\n",
        "    train_Dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=train_subsampler,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    sampler = utils.testSampler(val_data.cumulative_sizes[-1], config.outputLength)\n",
        "    sampler_test = utils.testSampler(test_data.cumulative_sizes[-1], config.outputLength)\n",
        "\n",
        "    vali_Dataloader = DataLoader(\n",
        "        val_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    test_Dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=config.batch_size,\n",
        "        sampler=sampler_test,\n",
        "        num_workers=1,\n",
        "        pin_memory=True)\n",
        "\n",
        "    logger.info(\"Initialize model\")\n",
        "    model = MAT(config).to(device)\n",
        "    logger.info(\"Model MAT\")\n",
        "\n",
        "    optim = optim.Adam(params=[p for p in model.parameters() if p.requires_grad], lr=config.lr)\n",
        "    net = Basic(model, optim)\n",
        "\n",
        "    # Resume from checkpoint if specified\n",
        "    if args.resume:\n",
        "        checkpoint_path = os.path.join(modelDir, args.checkpoint)\n",
        "        if os.path.exists(checkpoint_path):\n",
        "            logger.info(f\"Resuming training from checkpoint: {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            net = utils.loadModel(logger, net, checkpoint)\n",
        "        else:\n",
        "            logger.error(f\"Checkpoint file not found: {checkpoint_path}\")\n",
        "            logger.info(\"Starting training from scratch\")\n",
        "\n",
        "    criterion_r = nn.MSELoss()\n",
        "    criterion_c = nn.BCELoss()\n",
        "    criterion = [criterion_r, criterion_c]\n",
        "\n",
        "    logger.info(\"Training start\")\n",
        "    net_all = train(net, train_Dataloader, vali_Dataloader, config, criterion, modelDir, epo=epo)\n",
        "    logger.info(\"Training end\")\n",
        "\n",
        "    logger.info(\"validation start\")\n",
        "    utils.evaluateResult(net_all, config, vali_Dataloader, logger)\n",
        "    logger.info(\"test start\")\n",
        "    utils.evaluateResult(net_all, config, test_Dataloader, logger)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Using computation device: cuda:0\n",
            "2025-09-20 18:16:35,469 - root - INFO - Using computation device: cuda:0\n",
            "INFO:root:Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='test', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, resume=False, checkpoint='All_best_onoff.ckpt', output_dir='.')\n",
            "2025-09-20 18:16:35,470 - root - INFO - Namespace(batch=32, lr=0.001, dropout=0.1, hidden=32, logname='root', subName='test', inputLength=864, outputLength=864, debug=False, dataAug=False, prob0=0.3, prob1=0.6, prob2=0.3, prob3=0.3, resume=False, checkpoint='All_best_onoff.ckpt', output_dir='.')\n",
            "INFO:root:loading data\n",
            "2025-09-20 18:16:35,472 - root - INFO - loading data\n",
            "INFO:root:loading data finished\n",
            "2025-09-20 18:16:35,520 - root - INFO - loading data finished\n",
            "INFO:root:Training size: 27937.\n",
            "2025-09-20 18:16:35,529 - root - INFO - Training size: 27937.\n",
            "INFO:root:Initialize model\n",
            "2025-09-20 18:16:35,531 - root - INFO - Initialize model\n",
            "INFO:root:Model MAT\n",
            "2025-09-20 18:16:35,800 - root - INFO - Model MAT\n",
            "INFO:root:Training start\n",
            "2025-09-20 18:16:44,563 - root - INFO - Training start\n",
            "INFO:root:# of epoches: 0\n",
            "2025-09-20 18:16:44,565 - root - INFO - # of epoches: 0\n",
            "  0%|          | 0/874 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:366: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
            "  return F.conv1d(\n",
            "  6%|â–Œ         | 51/874 [00:59<16:29,  1.20s/it]"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMfU9SMFY9BlkHJKAN8+CNa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}